{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martinpella/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os, string, collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import utils\n",
    "from utils import *\n",
    "\n",
    "import snowballstemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten, Dense, Dropout, Convolution1D, MaxPooling1D, SpatialDropout1D, Input \n",
    "from keras.layers import GlobalMaxPooling1D, concatenate, LSTM, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{PATH}/data/Airline-Sentiment-2-w-AA.csv', usecols=['text', 'airline_sentiment'], encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode categorical label class into numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['target'] = le.fit_transform(df['airline_sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tc = TextCleaner()\n",
    "df['clean_text'] = tc.transform(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "\n",
    "def tokenize(s): \n",
    "    return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['tokenized'] = df['clean_text'].apply(lambda row: tokenize(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords removing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "stop.update(['amp', 'rt', 'cc'])\n",
    "stop = stop - set(['no', 'not'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(row):\n",
    "    return [t for t in row if t not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['tokenized'] = df['tokenized'].apply(lambda row: remove_stopwords(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>[said]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>[plus, youve, added, commercials, experience, tacky]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>[didnt, today, must, mean, need, take, another, trip]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "      <td>[really, aggressive, blast, obnoxious, entertainment, guests, faces, little, recourse]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>[really, big, bad, thing]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                             text  \\\n",
       "0  @VirginAmerica What @dhepburn said.                                                                                              \n",
       "1  @VirginAmerica plus you've added commercials to the experience... tacky.                                                         \n",
       "2  @VirginAmerica I didn't today... Must mean I need to take another trip!                                                          \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse   \n",
       "4  @VirginAmerica and it's a really big bad thing about it                                                                          \n",
       "\n",
       "                                                                                tokenized  \n",
       "0  [said]                                                                                  \n",
       "1  [plus, youve, added, commercials, experience, tacky]                                    \n",
       "2  [didnt, today, must, mean, need, take, another, trip]                                   \n",
       "3  [really, aggressive, blast, obnoxious, entertainment, guests, faces, little, recourse]  \n",
       "4  [really, big, bad, thing]                                                               "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text', 'tokenized']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_vocab_counter(row):\n",
    "    for word in row:\n",
    "        vocab_counter[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_counter = collections.Counter()\n",
    "df['tokenized'].apply(update_vocab_counter);\n",
    "vocab = sorted(vocab_counter, key=vocab_counter.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12390"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We limit the dictionary size to the top 5000 most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary that map each token with their id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2id = {w:i for i, w in enumerate(vocab[:max_words])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will replace each token out of top 5000 with 'unk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2id['unk'] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform each token by their id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_to_ids(row):\n",
    "    return [w2id[w] if w in w2id else w2id['unk'] for w in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['tokenized_int'] = df['tokenized'].apply(lambda x: transform_to_ids(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lens = df['tokenized_int'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 21, 8.987636612021857)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(lens), max(lens), np.mean(lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set 20 as max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['tokenized_int'].values, df['target'].values, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need that each document contains a fixed number of tokens (20), we fill with -1 (id that represents 'unk') every token with size < 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = pad_sequences(X_train, maxlen=maxlen, value=-1)\n",
    "x_test = pad_sequences(X_test, maxlen=maxlen, value=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We one-hot encode target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_y = np_utils.to_categorical(y_train)\n",
    "dummy_y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first approach is to create a neural network with a 50 dimension embedding layer as input and no hidden layers. This is equivalent as apply logistic regression over word vectors rather than one hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential([Embedding(input_dim=max_words, output_dim=50, input_length=maxlen),\n",
    "                        Flatten(),\n",
    "                        Dense(3, activation='softmax')])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the models we use cross-validation with 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=5, batch_size=100, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78.96134572269361, 0.815186961410359)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()*100, results.std()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train over full training set evaluating on test set. Best epoch is saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = f'{PATH}/results/linear.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10980 samples, validate on 3660 samples\n",
      "Epoch 1/5\n",
      "10600/10980 [===========================>..] - ETA: 0s - loss: 0.9084 - acc: 0.6273Epoch 00000: val_acc improved from -inf to 0.68470, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/linear.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.9025 - acc: 0.6282 - val_loss: 0.7158 - val_acc: 0.6847\n",
      "Epoch 2/5\n",
      "10600/10980 [===========================>..] - ETA: 0s - loss: 0.6111 - acc: 0.7443Epoch 00001: val_acc improved from 0.68470 to 0.76202, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/linear.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.6083 - acc: 0.7462 - val_loss: 0.5966 - val_acc: 0.7620\n",
      "Epoch 3/5\n",
      "10600/10980 [===========================>..] - ETA: 0s - loss: 0.4703 - acc: 0.8293Epoch 00002: val_acc improved from 0.76202 to 0.78798, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/linear.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.4678 - acc: 0.8311 - val_loss: 0.5458 - val_acc: 0.7880\n",
      "Epoch 4/5\n",
      "10600/10980 [===========================>..] - ETA: 0s - loss: 0.3701 - acc: 0.8753Epoch 00003: val_acc improved from 0.78798 to 0.79344, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/linear.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.3699 - acc: 0.8753 - val_loss: 0.5309 - val_acc: 0.7934\n",
      "Epoch 5/5\n",
      "10700/10980 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.9063Epoch 00004: val_acc did not improve\n",
      "10980/10980 [==============================] - 0s - loss: 0.2978 - acc: 0.9056 - val_loss: 0.5359 - val_acc: 0.7874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7de81a4f98>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = baseline_model()\n",
    "model.fit(x_train, dummy_y, validation_data=(x_test, dummy_y_test), epochs=5, batch_size=100, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple convolutional neural network with 1 conv layer (10 filters of size 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_model():\n",
    "    model = Sequential([Embedding(input_dim=max_words, output_dim=32, input_length=maxlen),\n",
    "                        Convolution1D(10, 3, padding='same', activation='relu'),\n",
    "                        MaxPooling1D(),\n",
    "                        Flatten(),\n",
    "                        Dense(50, activation='relu'),\n",
    "                        Dense(3, activation='softmax')])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=conv_model, epochs=5, batch_size=100, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76.43082918098617, 2.256687530027816)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()*100, results.std()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing number of filters to 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_model():\n",
    "    model = Sequential([Embedding(input_dim=max_words, output_dim=32, input_length=maxlen),\n",
    "                        Convolution1D(64, 3, padding='same', activation='relu'),\n",
    "                        MaxPooling1D(),\n",
    "                        Flatten(),\n",
    "                        Dense(25, activation='relu'),\n",
    "                        Dense(3, activation='softmax')])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=conv_model, epochs=5, batch_size=100, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77.98683471377974, 0.9380869930210952)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()*100, results.std()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a neural network that applies different filter sizes (2, 3 and 4) to word vectors, then concatenate the outputs and apply a classifier on top on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mult_conv():\n",
    "    graph_in = Input(shape=(max_words, 50))\n",
    "\n",
    "    convs = []\n",
    "    for filter_size in range(2, 5):\n",
    "        x = Convolution1D(64, filter_size, padding='same', activation='relu')(graph_in)\n",
    "        convs.append(x)\n",
    "\n",
    "    graph_out = concatenate(convs, axis=1)\n",
    "    graph_out = GlobalMaxPooling1D()(graph_out)\n",
    "    graph = Model(graph_in, graph_out)\n",
    "    \n",
    "    model = Sequential([Embedding(max_words, 50, input_length=maxlen),\n",
    "                    graph,\n",
    "                    Dropout(0.5),\n",
    "                    Dense(25, activation='relu'),\n",
    "                    Dense(3, activation='softmax')])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=mult_conv, epochs=5, batch_size=100, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79.02512735902401, 0.7688410157779406)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()*100, results.std()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = f'{PATH}/results/mult_conv.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10980 samples, validate on 3660 samples\n",
      "Epoch 1/5\n",
      "10000/10980 [==========================>...] - ETA: 0s - loss: 0.8885 - acc: 0.6051Epoch 00000: val_acc improved from -inf to 0.66776, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/mult_conv.hdf5\n",
      "10980/10980 [==============================] - 2s - loss: 0.8752 - acc: 0.6102 - val_loss: 0.7268 - val_acc: 0.6678\n",
      "Epoch 2/5\n",
      "10100/10980 [==========================>...] - ETA: 0s - loss: 0.6250 - acc: 0.7346Epoch 00001: val_acc improved from 0.66776 to 0.77596, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/mult_conv.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.6208 - acc: 0.7376 - val_loss: 0.5656 - val_acc: 0.7760\n",
      "Epoch 3/5\n",
      "10300/10980 [===========================>..] - ETA: 0s - loss: 0.4895 - acc: 0.8247Epoch 00002: val_acc improved from 0.77596 to 0.78880, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/mult_conv.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.4906 - acc: 0.8230 - val_loss: 0.5369 - val_acc: 0.7888\n",
      "Epoch 4/5\n",
      "10500/10980 [===========================>..] - ETA: 0s - loss: 0.3951 - acc: 0.8585Epoch 00003: val_acc improved from 0.78880 to 0.79699, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/mult_conv.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.3951 - acc: 0.8587 - val_loss: 0.5599 - val_acc: 0.7970\n",
      "Epoch 5/5\n",
      "10600/10980 [===========================>..] - ETA: 0s - loss: 0.3229 - acc: 0.8866Epoch 00004: val_acc did not improve\n",
      "10980/10980 [==============================] - 0s - loss: 0.3246 - acc: 0.8858 - val_loss: 0.5823 - val_acc: 0.7915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7d64be6518>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mult_conv()\n",
    "model.fit(x_train, dummy_y, validation_data=(x_test, dummy_y_test), epochs=5, batch_size=100, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(f'{PATH}/results/mult_conv.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7969945355191257"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, np.argmax(preds, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.92      0.87      2327\n",
      "          1       0.66      0.51      0.58       772\n",
      "          2       0.79      0.67      0.73       561\n",
      "\n",
      "avg / total       0.79      0.80      0.79      3660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, np.argmax(preds, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_lstm():\n",
    "    model = Sequential([Embedding(max_words, 50, input_length=maxlen),\n",
    "                        LSTM(25),\n",
    "                        Dense(3, activation='softmax')])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=simple_lstm, epochs=5, batch_size=100, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78.06884338406488, 0.8922790226571174)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()*100, results.std()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load pre-trained word embeddings from GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_dir = '/home/martinpella/Downloads/GloVe/Wikipedia2014'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(glove_dir + '/glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_emb_matrix(max_words, embedding_dim):\n",
    "    embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "    found = 0\n",
    "    for word, i in w2id.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if i < max_words:\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                found += 1\n",
    "            else:\n",
    "                embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "    return embedding_matrix, found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix, found = create_emb_matrix(max_words, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4676"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential([Embedding(input_dim=max_words, output_dim=50, input_length=maxlen),\n",
    "                        Flatten(),\n",
    "                        Dense(3, activation='softmax')])\n",
    "\n",
    "    model.layers[0].set_weights = [embedding_matrix]\n",
    "    model.layers[0].trainable = True\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=5, batch_size=100, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78.79746936570686, 0.7629237463005589)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()*100, results.std()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filepath = f'{PATH}/results/linear_glove.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10980 samples, validate on 3660 samples\n",
      "Epoch 1/5\n",
      "10300/10980 [===========================>..] - ETA: 0s - loss: 0.9241 - acc: 0.6159Epoch 00000: val_acc improved from -inf to 0.67869, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/linear_glove.hdf5\n",
      "10980/10980 [==============================] - 2s - loss: 0.9121 - acc: 0.6187 - val_loss: 0.7213 - val_acc: 0.6787\n",
      "Epoch 2/5\n",
      "10300/10980 [===========================>..] - ETA: 0s - loss: 0.6166 - acc: 0.7404Epoch 00001: val_acc improved from 0.67869 to 0.75710, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/linear_glove.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.6138 - acc: 0.7420 - val_loss: 0.5987 - val_acc: 0.7571\n",
      "Epoch 3/5\n",
      "10300/10980 [===========================>..] - ETA: 0s - loss: 0.4713 - acc: 0.8212Epoch 00002: val_acc improved from 0.75710 to 0.78415, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/linear_glove.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.4717 - acc: 0.8214 - val_loss: 0.5473 - val_acc: 0.7842\n",
      "Epoch 4/5\n",
      "10300/10980 [===========================>..] - ETA: 0s - loss: 0.3734 - acc: 0.8757Epoch 00003: val_acc improved from 0.78415 to 0.79262, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/linear_glove.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.3723 - acc: 0.8745 - val_loss: 0.5311 - val_acc: 0.7926\n",
      "Epoch 5/5\n",
      "10300/10980 [===========================>..] - ETA: 0s - loss: 0.2997 - acc: 0.9046Epoch 00004: val_acc did not improve\n",
      "10980/10980 [==============================] - 0s - loss: 0.2987 - acc: 0.9054 - val_loss: 0.5351 - val_acc: 0.7913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7d5aef5a90>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = baseline_model()\n",
    "model.fit(x_train, dummy_y, validation_data=(x_test, dummy_y_test), epochs=5, batch_size=100, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "#### Multiple Conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def mult_conv():\n",
    "    graph_in = Input(shape=(max_words, 100))\n",
    "\n",
    "    convs = []\n",
    "    for filter_size in range(2, 5):\n",
    "        x = Convolution1D(64, filter_size, padding='same', activation='relu')(graph_in)\n",
    "        convs.append(x)\n",
    "\n",
    "    graph_out = concatenate(convs, axis=1)\n",
    "    graph_out = GlobalMaxPooling1D()(graph_out)\n",
    "    graph = Model(graph_in, graph_out)\n",
    "    \n",
    "    model = Sequential([Embedding(max_words, 100, input_length=maxlen),\n",
    "                    graph,\n",
    "                    Dropout(0.5),\n",
    "                    Dense(25, activation='relu'),\n",
    "                    Dense(3, activation='softmax')])\n",
    "    \n",
    "    model.layers[0].set_weights = [embedding_matrix]\n",
    "    model.layers[0].trainable = True\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=mult_conv, epochs=5, batch_size=100, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78.87934992381346, 0.8683407303678339)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()*100, results.std()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_lstm():\n",
    "    model = Sequential([Embedding(max_words, 50, input_length=maxlen),\n",
    "                        LSTM(25),\n",
    "                        Dense(3, activation='softmax')])\n",
    "\n",
    "    model.layers[0].set_weights = [embedding_matrix]\n",
    "    model.layers[0].trainable = True\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=simple_lstm, epochs=5, batch_size=100, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78.37855569354937, 0.6784259807207694)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()*100, results.std()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = f'{PATH}/results/lstm_glove.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10980 samples, validate on 3660 samples\n",
      "Epoch 1/5\n",
      "10900/10980 [============================>.] - ETA: 0s - loss: 0.8159 - acc: 0.6480Epoch 00000: val_acc improved from -inf to 0.74098, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/lstm_glove.hdf5\n",
      "10980/10980 [==============================] - 8s - loss: 0.8144 - acc: 0.6487 - val_loss: 0.6448 - val_acc: 0.7410\n",
      "Epoch 2/5\n",
      "10900/10980 [============================>.] - ETA: 0s - loss: 0.5410 - acc: 0.7998Epoch 00001: val_acc improved from 0.74098 to 0.77842, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/lstm_glove.hdf5\n",
      "10980/10980 [==============================] - 2s - loss: 0.5404 - acc: 0.8002 - val_loss: 0.5651 - val_acc: 0.7784\n",
      "Epoch 3/5\n",
      "10900/10980 [============================>.] - ETA: 0s - loss: 0.3968 - acc: 0.8510Epoch 00002: val_acc improved from 0.77842 to 0.77896, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/lstm_glove.hdf5\n",
      "10980/10980 [==============================] - 2s - loss: 0.3967 - acc: 0.8510 - val_loss: 0.5470 - val_acc: 0.7790\n",
      "Epoch 4/5\n",
      "10900/10980 [============================>.] - ETA: 0s - loss: 0.3175 - acc: 0.8855Epoch 00003: val_acc improved from 0.77896 to 0.78333, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/lstm_glove.hdf5\n",
      "10980/10980 [==============================] - 2s - loss: 0.3186 - acc: 0.8848 - val_loss: 0.5759 - val_acc: 0.7833\n",
      "Epoch 5/5\n",
      "10900/10980 [============================>.] - ETA: 0s - loss: 0.2679 - acc: 0.9047Epoch 00004: val_acc did not improve\n",
      "10980/10980 [==============================] - 2s - loss: 0.2687 - acc: 0.9043 - val_loss: 0.6196 - val_acc: 0.7806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7d1ed61f28>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = simple_lstm()\n",
    "model.fit(x_train, dummy_y, validation_data=(x_test, dummy_y_test), epochs=5, batch_size=100, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(f'{PATH}/results/lstm_glove.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7833333333333333"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, np.argmax(preds, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.88      0.86      2327\n",
      "          1       0.60      0.58      0.59       772\n",
      "          2       0.74      0.67      0.70       561\n",
      "\n",
      "avg / total       0.78      0.78      0.78      3660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, np.argmax(preds, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bi_lstm():\n",
    "    model = Sequential([Embedding(max_words, 50, input_length=maxlen),\n",
    "                        Bidirectional(LSTM(25, dropout=0.2, recurrent_dropout=0.2)),\n",
    "                        Dense(3, activation='softmax')])\n",
    "\n",
    "    model.layers[0].set_weights = [embedding_matrix]\n",
    "    model.layers[0].trainable = True\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=bi_lstm, epochs=5, batch_size=100, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78.4420255733871, 1.164233643944509)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()*100, results.std()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7833333333333333"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, np.argmax(preds, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.88      0.86      2327\n",
      "          1       0.60      0.58      0.59       772\n",
      "          2       0.74      0.67      0.70       561\n",
      "\n",
      "avg / total       0.78      0.78      0.78      3660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, np.argmax(preds, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def double_lstm():\n",
    "    model = Sequential([Embedding(max_words, 50, input_length=maxlen),\n",
    "                        LSTM(50, dropout=0.2, recurrent_dropout=0.2),\n",
    "                        Dense(3, activation='softmax')])\n",
    "\n",
    "    model.layers[0].set_weights = [embedding_matrix]\n",
    "    model.layers[0].trainable = True\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=double_lstm, epochs=5, batch_size=100, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78.28734835713527, 1.04602968811997)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()*100, results.std()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
